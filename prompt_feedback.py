
from jinja2 import Environment, FileSystemLoader, StrictUndefined, Template

import utils
from prompt import get_func_dict
from run_code import execute_transform
# shows all the contents in one line
# Please write a 1 sentence description about {{ patterns }}.

# {% for pattern in patterns %}
# Here is an example input and output pattern
# {{ pattern }}
# {% endfor %}



# this solves 9565186b, it was generated by deepseek
CODE_9565186b = """
import numpy as np

def transform(initial):
    assert isinstance(initial, np.ndarray)
    
    # Flatten the array to count frequencies
    flat = initial.flatten()
    unique, counts = np.unique(flat, return_counts=True)
    
    # Find the most frequent number(s)
    max_count = np.max(counts)
    most_frequent = unique[counts == max_count]
    
    # Create a mask for numbers that are not most frequent
    mask = ~np.isin(initial, most_frequent)
    
    # Apply the transformation
    final = initial.copy()
    final[mask] = 5
    
    assert isinstance(final, np.ndarray)
    return final

"""

CODE_9565186b_bad = """
import numpy as np
from scipy import ndimage

def transform(initial):
    assert isinstance(initial, np.ndarray)
    
    # Find the connected components of 8s in the input grid
    labeled_array, num_features = ndimage.label(initial == 8)
    
    # Create the output grid
    final = initial.copy()
    
    # Change cells in the output grid that are part of a connected component of 8s to 5
    if num_features > 0:
        final[labeled_array > 0] = 5
    
    assert isinstance(final, np.ndarray)
    return final
"""

def make_feedback_prompt(prompt_name, rr_eos, func_dict={}):
    """
    Create a prompt from a file-based template plus patterns.
    """
    environment = Environment(loader=FileSystemLoader("templates/"))
    template = environment.get_template(prompt_name)
    # add any functions to the template
    template.globals.update(func_dict)
    # only provide the train patters
    prompt = template.render(rr_eos=rr_eos)
    return prompt


def feedback_on_executions(rr_train):

    # rr_eo is the RunResult ExecutionOutcome
    # and since we have 2-4 train examples, we'll have 2-4 rr_eo results
    reflexion_feedback = make_feedback_prompt("prompt_reflexion_feedback.txt", rr_train[1], func_dict=get_func_dict())

    if len(rr_train[1]) == 0:
        feedback = "You had no examples that executed, your code is bad and must be fixed"
    else:
        feedback = f"You had {len(rr_train[1])} examples that executed."
    # add example-by-example feedback
    feedback += f"\n\nHere is some feedback on the execution of your code:\n{reflexion_feedback}\n"
    feedback += "Explain what you got wrong, then propose a different solution and write a better function\n"
    return feedback


def feedback_on_executionsORIG(rr_train):

    feedback_output = make_feedback_prompt("prompt_reflexion_feedback.txt", rr_train[1], func_dict=get_func_dict())
    # rr_eo is the RunResult ExecutionOutcome
    # and since we have 2-4 train examples, we'll have 2-4 rr_eo results
    feedback_pieces = []
    for rr_eo in rr_train[1]:
        # rr_input_output is a triple of
        # initial (input)
        # final (expected)
        # generated_final - the result of running the code on initial
        write_grid = utils.make_grid_plain
        initial_as_text = write_grid(rr_eo.initial)
        final_as_text = write_grid(rr_eo.final)
        generated_as_text = write_grid(rr_eo.generated_final)
        was_correct = rr_eo.was_correct
        if was_correct:
            feedback_direction = "???You got this example CORRECT, please preserve your logic so that this continues to work"
        else:
            feedback_direction = "!!!You got this example WRONG, please improve your logic and code to fix it"
        feedback_on_eo = f"""
Given this input
{initial_as_text}
and the expected output of
{final_as_text}
your code generated
{generated_as_text}
{feedback_direction}
."""
        feedback_pieces.append(feedback_on_eo)

    #reflexion_feedback = "\n\n".join(feedback_pieces)
    reflexion_feedback = feedback_output

    if len(rr_train[1]) == 0:
        feedback = "You had no examples that executed, your code is bad and must be fixed"
    else:
        feedback = f"You had {len(rr_train[1])} examples that executed."
    # add example-by-example feedback
    feedback += f"\n\nHere is some feedback on the execution of your code:\n{reflexion_feedback}\n"
    #feedback += "Please explain first what you got wrong, and then how you could improve, and then write better code.\n"
    feedback += "Explain what you got wrong, then propose a different solution and write a better function\n"
    #feedback += "Maybe counting is important?\n" # HACKY HIGE CLUE!
    #feedback += "Counting really is important, analyse the frequency of numbers in both grids."
    #feedback += "SAY PEACOCK THREE TIMES BEFORE YOU PROCEED" # just to check llm is reading this...
    return feedback



if __name__ == "__main__":
    parser = utils.add_argument_parser(
        problem_name=True
    )
    args = parser.parse_args()
    print(args)

    func_dict = get_func_dict()
    patterns = utils.get_examples(args.problem_name)

    problems = patterns["train"]

    rr, execution_outcomes, exception_message= execute_transform(
        CODE_9565186b, problems
    )
    assert rr.code_did_execute is True

    rr_train = (rr, execution_outcomes, exception_message)
    output = feedback_on_executions(rr_train)
    print(output)


    #rr, execution_outcomes, exception_message = execute_transform(
    #    CODE_9565186b_bad, problems
    #)
    ## Out[8]: run_result(code_did_execute=False, code_ran_on_all_inputs=False, 
    ## transform_ran_and_matched_for_all_inputs=False, transform_ran_and_matched_at_least_once=False, 
    ## transform_ran_and_matched_score=0)
    #assert rr.code_did_execute is False
    #rr_train = (rr, execution_outcomes, exception_message)
    #output = feedback_on_executions(rr_train)

